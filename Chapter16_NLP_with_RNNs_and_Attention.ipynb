{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e80d4b7",
      "metadata": {
        "id": "1e80d4b7"
      },
      "source": [
        "\n",
        "# Chapter 16: Natural Language Processing with RNNs and Attention\n",
        "\n",
        "## 1. Pendahuluan\n",
        "\n",
        "Natural Language Processing (NLP) adalah cabang Machine Learning yang menangani data berbentuk teks atau bahasa manusia. Pada bab ini, fokusnya adalah memahami bagaimana menggunakan Recurrent Neural Networks (RNNs) dan Attention Mechanisms untuk memproses teks secara urutan, menerjemahkan bahasa, menganalisis sentimen, dan tugas NLP lainnya.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b94426a9",
      "metadata": {
        "id": "b94426a9"
      },
      "source": [
        "\n",
        "## 2. Tokenization dan Encoding\n",
        "\n",
        "Langkah pertama pada NLP adalah mengubah teks mentah menjadi token numerik agar dapat diproses oleh model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec790d7f",
      "metadata": {
        "id": "ec790d7f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    \"I love TensorFlow\",\n",
        "    \"NLP is amazing\"\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1400d05",
      "metadata": {
        "id": "b1400d05"
      },
      "source": [
        "\n",
        "## 3. Padding Sequences\n",
        "\n",
        "Agar data dapat diproses dalam batch, semua urutan harus memiliki panjang yang sama. Gunakan `pad_sequences`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "777670d3",
      "metadata": {
        "id": "777670d3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded = pad_sequences(sequences, padding='post')\n",
        "print(padded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42a9c15",
      "metadata": {
        "id": "b42a9c15"
      },
      "source": [
        "\n",
        "## 4. Word Embeddings\n",
        "\n",
        "Word Embedding mengubah token menjadi vektor numerik padat yang menangkap makna semantik kata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3930f7fc",
      "metadata": {
        "id": "3930f7fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=100, output_dim=16, input_length=5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d01aa37",
      "metadata": {
        "id": "4d01aa37"
      },
      "source": [
        "\n",
        "## 5. Recurrent Neural Networks (RNN)\n",
        "\n",
        "RNN adalah arsitektur khusus untuk memproses data urutan. `SimpleRNN`, `LSTM`, dan `GRU` adalah varian RNN di Keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "759f4a5f",
      "metadata": {
        "id": "759f4a5f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
        "    LSTM(64),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563043e0",
      "metadata": {
        "id": "563043e0"
      },
      "source": [
        "\n",
        "## 6. Bidirectional RNN\n",
        "\n",
        "RNN standar hanya membaca data dari awal ke akhir. `Bidirectional` memungkinkan membaca maju dan mundur sehingga konteks lebih kaya.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b4d229a",
      "metadata": {
        "id": "3b4d229a"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f6b503",
      "metadata": {
        "id": "76f6b503"
      },
      "source": [
        "\n",
        "## 7. Sequence-to-Sequence (Seq2Seq)\n",
        "\n",
        "Seq2Seq digunakan pada tugas terjemahan, summarization, dan chatbot. Arsitektur umumnya melibatkan Encoder dan Decoder.\n",
        "\n",
        "Encoder: Membaca input sequence.  \n",
        "Decoder: Menghasilkan output sequence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ac9aaa",
      "metadata": {
        "id": "45ac9aaa"
      },
      "source": [
        "\n",
        "## 8. Attention Mechanism\n",
        "\n",
        "Attention meningkatkan performa Seq2Seq dengan cara memungkinkan decoder untuk melihat ke seluruh input sequence dan fokus pada bagian relevan saat menghasilkan setiap token output.\n",
        "\n",
        "Attention dapat diintegrasikan sebagai layer khusus. TensorFlow Addons atau implementasi manual dapat digunakan untuk menambahkan Attention.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12d166d0",
      "metadata": {
        "id": "12d166d0"
      },
      "source": [
        "\n",
        "## 9. Transformer Architecture\n",
        "\n",
        "Transformer adalah arsitektur modern untuk NLP, menggantikan RNN dengan Attention sepenuhnya. Komponen utamanya adalah Self-Attention dan Multi-Head Attention.\n",
        "\n",
        "Model terkenal berbasis Transformer:\n",
        "- BERT (Bidirectional Encoder Representations from Transformers)\n",
        "- GPT (Generative Pre-trained Transformer)\n",
        "\n",
        "Transformer mendukung paralelisasi dan performa lebih cepat dibanding RNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0283ee0",
      "metadata": {
        "id": "a0283ee0"
      },
      "source": [
        "\n",
        "## 10. Fine-Tuning Pretrained NLP Models\n",
        "\n",
        "Transfer learning juga populer di NLP. Model BERT, GPT, T5 dan lainnya dapat di-*fine-tune* menggunakan `transformers` library dari Hugging Face.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71092405",
      "metadata": {
        "id": "71092405"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c17f3fe1",
      "metadata": {
        "id": "c17f3fe1"
      },
      "source": [
        "\n",
        "## 11. Tips Praktis NLP\n",
        "\n",
        "- Selalu bersihkan data teks: hilangkan stopwords, tanda baca, dan normalisasi casing.\n",
        "- Gunakan embedding pretrained (misal GloVe, Word2Vec) jika dataset kecil.\n",
        "- Untuk dataset besar, training embedding dari awal juga bisa efektif.\n",
        "- Jika memungkinkan, gunakan model Transformer karena performanya lebih baik dibanding RNN pada umumnya.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
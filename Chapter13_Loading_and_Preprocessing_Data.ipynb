{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "71fd9ff7",
      "metadata": {
        "id": "71fd9ff7"
      },
      "source": [
        "\n",
        "#  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
        "\n",
        "## 1. Pendahuluan\n",
        "\n",
        "Pada bab ini, kita belajar cara memuat dan memproses data dalam pipeline yang efisien menggunakan **TensorFlow Data API**. Pendekatan ini sangat penting untuk melatih model skala besar secara cepat.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0d84b5",
      "metadata": {
        "id": "5c0d84b5"
      },
      "source": [
        "\n",
        "## 2. TensorFlow Data API\n",
        "\n",
        "`tf.data` API digunakan untuk membangun pipeline input data yang scalable, reusable, dan efisien.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14eed8eb",
      "metadata": {
        "id": "14eed8eb"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "dataset = tf.data.Dataset.range(10)\n",
        "for item in dataset:\n",
        "    print(item.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deda9218",
      "metadata": {
        "id": "deda9218"
      },
      "source": [
        "\n",
        "## 3. Chaining Transformations\n",
        "\n",
        "Dataset mendukung *method chaining* seperti `map`, `filter`, dan `batch`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6af6cf6e",
      "metadata": {
        "id": "6af6cf6e"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.map(lambda x: x * 2)\n",
        "dataset = dataset.filter(lambda x: x < 10)\n",
        "dataset = dataset.batch(3)\n",
        "\n",
        "for batch in dataset:\n",
        "    print(batch.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a606e57c",
      "metadata": {
        "id": "a606e57c"
      },
      "source": [
        "\n",
        "## 4. Shuffle dan Repeat\n",
        "\n",
        "Untuk *stochastic training*, data perlu di-*shuffle*:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fdf917",
      "metadata": {
        "id": "49fdf917"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.shuffle(buffer_size=5).repeat(2).batch(3)\n",
        "\n",
        "for batch in dataset:\n",
        "    print(batch.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "413f2951",
      "metadata": {
        "id": "413f2951"
      },
      "source": [
        "\n",
        "## 5. Preprocessing Data\n",
        "\n",
        "Pipeline dapat melakukan preprocessing, seperti normalisasi:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff67bf0",
      "metadata": {
        "id": "9ff67bf0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalize(x):\n",
        "    return (x - tf.reduce_mean(x)) / tf.math.reduce_std(x)\n",
        "\n",
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.map(normalize)\n",
        "\n",
        "for item in dataset:\n",
        "    print(item.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3826d6e",
      "metadata": {
        "id": "b3826d6e"
      },
      "source": [
        "\n",
        "## 6. Prefetching\n",
        "\n",
        "Gunakan `prefetch` untuk overlap loading & execution, meningkatkan performa GPU:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50fa423",
      "metadata": {
        "id": "a50fa423"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.prefetch(buffer_size=1)\n",
        "\n",
        "for item in dataset:\n",
        "    print(item.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4a22920",
      "metadata": {
        "id": "c4a22920"
      },
      "source": [
        "\n",
        "## 7. Menggunakan Dataset dengan Keras\n",
        "\n",
        "Dataset dapat langsung dipakai di `model.fit`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1858312e",
      "metadata": {
        "id": "1858312e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Contoh pseudo-code\n",
        "# model.compile(...)\n",
        "# model.fit(dataset, epochs=5, steps_per_epoch=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd82e9f4",
      "metadata": {
        "id": "cd82e9f4"
      },
      "source": [
        "\n",
        "## 8. Format TFRecord\n",
        "\n",
        "Untuk dataset besar, format binary `TFRecord` lebih cepat dari file CSV.\n",
        "\n",
        "### Menulis TFRecord:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "452fdeaf",
      "metadata": {
        "id": "452fdeaf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# with tf.io.TFRecordWriter(\"my_data.tfrecord\") as writer:\n",
        "#     for record in records:\n",
        "#         writer.write(record.SerializeToString())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fadc7cb",
      "metadata": {
        "id": "0fadc7cb"
      },
      "source": [
        "\n",
        "### Membaca TFRecord:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bea5701",
      "metadata": {
        "id": "7bea5701"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dataset = tf.data.TFRecordDataset([\"my_data.tfrecord\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57dc18a8",
      "metadata": {
        "id": "57dc18a8"
      },
      "source": [
        "\n",
        "## 9. Protocol Buffers\n",
        "\n",
        "TFRecord menyimpan data dalam format **Protocol Buffers** (`protobuf`). Protobuf adalah format serialisasi yang efisien dan extensible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5161840",
      "metadata": {
        "id": "f5161840"
      },
      "source": [
        "\n",
        "## 10. Preprocessing Fitur Input\n",
        "\n",
        "**One-Hot Encoding** atau **Embedding** dapat digunakan untuk atribut kategorikal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18be0d27",
      "metadata": {
        "id": "18be0d27"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.layers import StringLookup, CategoryEncoding\n",
        "\n",
        "lookup = StringLookup(vocabulary=[\"A\", \"B\", \"C\"])\n",
        "one_hot = CategoryEncoding(num_tokens=3)\n",
        "\n",
        "strings = tf.constant([\"B\", \"A\", \"C\"])\n",
        "encoded = lookup(strings)\n",
        "one_hot_encoded = one_hot(encoded)\n",
        "\n",
        "print(one_hot_encoded.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4976131f",
      "metadata": {
        "id": "4976131f"
      },
      "source": [
        "\n",
        "## 11. TF Transform\n",
        "\n",
        "`TF Transform` dapat digunakan untuk preprocessing skala produksi yang konsisten antara training & serving.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be00c86",
      "metadata": {
        "id": "8be00c86"
      },
      "source": [
        "\n",
        "## 12. TensorFlow Datasets (TFDS)\n",
        "\n",
        "`tfds` menyediakan dataset siap pakai dengan satu baris kode:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2fdf8b",
      "metadata": {
        "id": "1b2fdf8b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset = tfds.load(\"mnist\", split=\"train\")\n",
        "print(dataset)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7412373b",
      "metadata": {
        "id": "7412373b"
      },
      "source": [
        "\n",
        "# Chapter 4: Training Models\n",
        "\n",
        "## 1. Pendahuluan\n",
        "\n",
        "Pada bab ini, kita akan mendalami bagaimana cara melatih model Machine Learning dengan benar, memahami rumus matematis di balik Linear Regression, Polynomial Regression, Logistic Regression, serta Softmax Regression. Bab ini juga menjelaskan algoritma optimasi seperti **Gradient Descent** beserta variannya.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e716b703",
      "metadata": {
        "id": "e716b703"
      },
      "source": [
        "\n",
        "## 2. Definisi dan Tujuan Model Machine Learning\n",
        "\n",
        "Tujuan dari melatih model adalah menemukan parameter terbaik yang meminimalkan **cost function**, sehingga model dapat mempelajari pola data dengan baik dan memiliki performa generalisasi yang optimal pada data baru.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dc71ad0",
      "metadata": {
        "id": "1dc71ad0"
      },
      "source": [
        "\n",
        "## 3. Linear Regression\n",
        "\n",
        "### 3.1 Model Prediksi Linear\n",
        "\n",
        "\\[\n",
        "$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n$\n",
        "\\]\n",
        "\n",
        "Dalam bentuk vektor:\n",
        "\n",
        "\\[\n",
        "$\\hat{y} = h_{\\theta}(x) = \\theta^T x$\n",
        "\\]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98cb88f2",
      "metadata": {
        "id": "98cb88f2"
      },
      "source": [
        "\n",
        "### 3.2 Normal Equation\n",
        "\n",
        "Untuk menemukan parameter optimal secara langsung:\n",
        "\n",
        "\\[\n",
        "$\\theta = (X^T X)^{-1} X^T y$\n",
        "\\]\n",
        "\n",
        "#### Contoh Kode:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416459f4",
      "metadata": {
        "id": "416459f4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Dataset dummy\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Tambahkan x0 = 1\n",
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "\n",
        "# Hitung theta dengan Normal Equation\n",
        "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "\n",
        "print(theta_best)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81e08f2c",
      "metadata": {
        "id": "81e08f2c"
      },
      "source": [
        "\n",
        "## 4. Gradient Descent\n",
        "\n",
        "Gradient Descent digunakan untuk meminimasi cost function dengan pendekatan iteratif.\n",
        "\n",
        "### 4.1 Cost Function (MSE)\n",
        "\n",
        "\\[\n",
        "$MSE(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (\\theta^T x^{(i)} - y^{(i)})^2$\n",
        "\\]\n",
        "\n",
        "### 4.2 Update Rule\n",
        "\n",
        "\\[\n",
        "$\\theta := \\theta - \\eta \\nabla_{\\theta} MSE(\\theta)$\n",
        "\\]\n",
        "\n",
        "### 4.3 Variasi Gradient Descent\n",
        "\n",
        "- **Batch Gradient Descent**\n",
        "- **Stochastic Gradient Descent (SGD)**\n",
        "- **Mini-batch Gradient Descent**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c7c4cc",
      "metadata": {
        "id": "b6c7c4cc"
      },
      "source": [
        "\n",
        "## 5. Polynomial Regression\n",
        "\n",
        "Untuk data yang non-linear, kita dapat menggunakan Polynomial Regression:\n",
        "\n",
        "\\[\n",
        "$y = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\ldots + \\theta_n x^n$\n",
        "\\]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aabe5ad",
      "metadata": {
        "id": "2aabe5ad"
      },
      "source": [
        "\n",
        "## 6. Learning Curves\n",
        "\n",
        "Digunakan untuk mendeteksi overfitting dan underfitting.\n",
        "\n",
        "### Contoh Fungsi Python:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c4da2b",
      "metadata": {
        "id": "53c4da2b"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "    train_errors, val_errors = [], []\n",
        "    for m in range(1, len(X_train)):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_val_predict = model.predict(X_val)\n",
        "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
        "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
        "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
        "    plt.xlabel(\"Training set size\")\n",
        "    plt.ylabel(\"RMSE\")\n",
        "    plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3414bf85",
      "metadata": {
        "id": "3414bf85"
      },
      "source": [
        "\n",
        "## 7. Regularized Linear Models\n",
        "\n",
        "Untuk menghindari overfitting:\n",
        "\n",
        "- **Ridge Regression (L2)**\n",
        "- **Lasso Regression (L1)**\n",
        "- **Elastic Net** (kombinasi L1 & L2)\n",
        "- **Early Stopping**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fabfb9a",
      "metadata": {
        "id": "9fabfb9a"
      },
      "source": [
        "\n",
        "## 8. Logistic Regression\n",
        "\n",
        "Untuk klasifikasi biner.\n",
        "\n",
        "\\[\n",
        "$p = \\sigma(t) = \\frac{1}{1 + e^{-t}}, \\quad t = \\theta^T x$\n",
        "\\]\n",
        "\n",
        "Cost Function:\n",
        "\n",
        "\\[\n",
        "$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [ y^{(i)} \\log p^{(i)} + (1 - y^{(i)}) \\log (1 - p^{(i)}) ]$\n",
        "\\]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b2d7743",
      "metadata": {
        "id": "7b2d7743"
      },
      "source": [
        "\n",
        "## 9. Softmax Regression\n",
        "\n",
        "Ekstensi Logistic Regression untuk multi-class classification.\n",
        "\n",
        "Softmax score:\n",
        "\n",
        "\\[\n",
        "$s_k(x) = \\theta_k^T x$\n",
        "\\]\n",
        "\n",
        "Probabilitas:\n",
        "\n",
        "\\[\n",
        "$\\hat{p}_k = \\frac{e^{s_k(x)}}{\\sum_{j=1}^{K} e^{s_j(x)}}$\n",
        "\\]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3275e6c0",
      "metadata": {
        "id": "3275e6c0"
      },
      "source": [
        "\n",
        "# Chapter 15: Distributing TensorFlow Across Devices and Servers\n",
        "\n",
        "## 1. Pendahuluan\n",
        "\n",
        "Pada bab ini, fokusnya adalah bagaimana memanfaatkan kemampuan distribusi TensorFlow untuk menjalankan training pada beberapa GPU, beberapa CPU, atau bahkan beberapa server (cluster). Hal ini sangat penting untuk mempercepat training model yang besar, memanfaatkan sumber daya secara efisien, dan memungkinkan eksperimen skala industri.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63b2a8af",
      "metadata": {
        "id": "63b2a8af"
      },
      "source": [
        "\n",
        "## 2. Strategi Distribusi\n",
        "\n",
        "TensorFlow menyediakan API `tf.distribute` untuk mempermudah penanganan distribusi training.\n",
        "\n",
        "Beberapa strategi umum:\n",
        "- `MirroredStrategy`: Training paralel pada beberapa GPU dalam satu mesin.\n",
        "- `MultiWorkerMirroredStrategy`: Training paralel pada beberapa worker node.\n",
        "- `TPUStrategy`: Optimasi untuk TPU.\n",
        "- `CentralStorageStrategy`: Bobot disimpan di CPU, worker di GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0457a06f",
      "metadata": {
        "id": "0457a06f"
      },
      "source": [
        "\n",
        "## 3. Menggunakan MirroredStrategy\n",
        "\n",
        "`MirroredStrategy` adalah strategi paling sederhana untuk distribusi multi-GPU dalam satu mesin.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df5adfa8",
      "metadata": {
        "id": "df5adfa8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
        "\n",
        "with strategy.scope():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(10)\n",
        "    ])\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38902f8c",
      "metadata": {
        "id": "38902f8c"
      },
      "source": [
        "\n",
        "## 4. MultiWorkerMirroredStrategy\n",
        "\n",
        "Jika training dilakukan di beberapa mesin (multi-worker cluster), gunakan `MultiWorkerMirroredStrategy`.\n",
        "\n",
        "Untuk menjalankan cluster, setiap worker harus memiliki environment variable `TF_CONFIG` yang mendefinisikan cluster topology.\n",
        "\n",
        "Contoh konfigurasi `TF_CONFIG` untuk 2 worker:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e46fa951",
      "metadata": {
        "id": "e46fa951"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Contoh JSON TF_CONFIG\n",
        "{\n",
        "  \"cluster\": {\n",
        "    \"worker\": [\"worker1.example.com:12345\", \"worker2.example.com:23456\"]\n",
        "  },\n",
        "  \"task\": {\"type\": \"worker\", \"index\": 0}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "191ba546",
      "metadata": {
        "id": "191ba546"
      },
      "source": [
        "\n",
        "## 5. ParameterServerStrategy\n",
        "\n",
        "`ParameterServerStrategy` digunakan untuk skenario arsitektur parameter server, yang cocok untuk model yang sangat besar. Bobot model disimpan di server terpusat, sedangkan worker menghitung gradien.\n",
        "\n",
        "Biasanya digunakan pada cluster dengan banyak node. Konsepnya mirip dengan framework distributed lain seperti Horovod.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed2e9146",
      "metadata": {
        "id": "ed2e9146"
      },
      "source": [
        "\n",
        "## 6. TPUStrategy\n",
        "\n",
        "Untuk memanfaatkan TPU, TensorFlow menyediakan `TPUStrategy` yang secara otomatis mengatur distribusi training di TPU core.\n",
        "\n",
        "Contoh dasar:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df0cd06",
      "metadata": {
        "id": "7df0cd06"
      },
      "outputs": [],
      "source": [
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "    # definisi model\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bfc8de2",
      "metadata": {
        "id": "1bfc8de2"
      },
      "source": [
        "\n",
        "## 7. Dataset Sharding\n",
        "\n",
        "Saat distribusi training, dataset perlu di-*shard* otomatis untuk setiap replica agar tidak ada overlap data.\n",
        "\n",
        "TensorFlow menangani hal ini secara otomatis jika dataset dibungkus di dalam scope `strategy`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20caf002",
      "metadata": {
        "id": "20caf002"
      },
      "outputs": [],
      "source": [
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "GLOBAL_BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.range(1000)\n",
        "dataset = dataset.batch(GLOBAL_BATCH_SIZE)\n",
        "\n",
        "dist_dataset = strategy.experimental_distribute_dataset(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e95959c4",
      "metadata": {
        "id": "e95959c4"
      },
      "source": [
        "\n",
        "## 8. Custom Training Loop dengan Distribusi\n",
        "\n",
        "Untuk custom loop, gunakan `strategy.run()` agar operasi dieksekusi di setiap replica.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "610fe3d1",
      "metadata": {
        "id": "610fe3d1"
      },
      "outputs": [],
      "source": [
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "    model = tf.keras.Sequential([...])\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs):\n",
        "    def step_fn(inputs):\n",
        "        data, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(data, training=True)\n",
        "            loss = loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    per_replica_losses = strategy.run(step_fn, args=(inputs,))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1029c19",
      "metadata": {
        "id": "c1029c19"
      },
      "source": [
        "\n",
        "## 9. Tips dan Best Practices\n",
        "\n",
        "- Gunakan batch size yang lebih besar jika menggunakan banyak device agar throughput optimal.\n",
        "- Pastikan data pipeline menggunakan `prefetch` dan `cache` untuk meminimalkan bottleneck I/O.\n",
        "- Monitor resource (GPU/CPU/Network) untuk mendeteksi bottleneck.\n",
        "- Tes distribusi di subset data dulu sebelum skala penuh.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
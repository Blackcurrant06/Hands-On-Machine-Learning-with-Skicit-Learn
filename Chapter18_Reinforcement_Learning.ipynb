{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f422f218",
      "metadata": {
        "id": "f422f218"
      },
      "source": [
        "\n",
        "# Chapter 18: Reinforcement Learning\n",
        "\n",
        "## 1. Pendahuluan\n",
        "\n",
        "Reinforcement Learning (RL) adalah cabang Machine Learning di mana agen belajar melalui interaksi dengan lingkungan untuk memaksimalkan cumulative reward. Berbeda dengan supervised learning yang memiliki label tetap, RL menggunakan trial and error.\n",
        "\n",
        "Contoh aplikasi RL:\n",
        "- Game AI (AlphaGo, Atari)\n",
        "- Robotik (navigasi, manipulasi)\n",
        "- Sistem rekomendasi adaptif\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75461f3",
      "metadata": {
        "id": "a75461f3"
      },
      "source": [
        "\n",
        "## 2. Terminologi Dasar\n",
        "\n",
        "Elemen dasar Reinforcement Learning:\n",
        "\n",
        "- Agent: Entitas yang belajar dan membuat keputusan.\n",
        "- Environment: Dunia tempat agent berinteraksi.\n",
        "- State (S): Situasi saat ini.\n",
        "- Action (A): Tindakan yang dapat diambil agent.\n",
        "- Reward (R): Nilai umpan balik dari environment.\n",
        "- Policy (π): Strategi yang menentukan aksi berdasarkan state.\n",
        "- Value Function (V): Estimasi reward jangka panjang dari suatu state.\n",
        "- Q-Value (Q): Estimasi reward jangka panjang dari kombinasi state-action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f67a21a5",
      "metadata": {
        "id": "f67a21a5"
      },
      "source": [
        "\n",
        "## 3. Markov Decision Process (MDP)\n",
        "\n",
        "RL sering diformalkan sebagai Markov Decision Process (MDP):\n",
        "- MDP didefinisikan sebagai tuple (S, A, P, R, γ)\n",
        "  - S: Set state\n",
        "  - A: Set action\n",
        "  - P(s'|s,a): Probabilitas transisi ke state s'\n",
        "  - R(s,a): Reward saat action a diambil di state s\n",
        "  - γ: Discount factor\n",
        "\n",
        "Tujuan: Temukan policy π yang memaksimalkan Expected Return.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eceb58c",
      "metadata": {
        "id": "7eceb58c"
      },
      "source": [
        "\n",
        "## 4. Bellman Equation\n",
        "\n",
        "Value Function didefinisikan:\n",
        "\n",
        "$V^π(s) = Eπ [ R_{t+1} + γ V^π(S_{t+1}) | S_t = s ]$\n",
        "\n",
        "Bellman Optimality Equation:\n",
        "\n",
        "$V*(s) = max_a E [ R_{t+1} + γ V*(S_{t+1}) | S_t = s, A_t = a ]$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad0c778",
      "metadata": {
        "id": "0ad0c778"
      },
      "source": [
        "\n",
        "## 5. Policy Iteration\n",
        "\n",
        "Policy Iteration adalah algoritma iteratif untuk menemukan policy optimal.\n",
        "\n",
        "Langkah:\n",
        "1. Policy Evaluation: Hitung $V^π$ untuk policy π\n",
        "2. Policy Improvement: Perbarui policy dengan memilih action terbaik berdasarkan $V^π$\n",
        "\n",
        "Ulangi sampai policy konvergen.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9b02df",
      "metadata": {
        "id": "ff9b02df"
      },
      "source": [
        "\n",
        "## 6. Value Iteration\n",
        "\n",
        "Alternatif Policy Iteration adalah Value Iteration: memperbarui Value Function langsung dengan Bellman Optimality.\n",
        "\n",
        "Update:\n",
        "\n",
        "$V_{k+1}(s) = max_a E [ R_{t+1} + γ V_k(S_{t+1}) ]$\n",
        "\n",
        "Value Iteration sering lebih cepat dari Policy Iteration untuk masalah besar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f73ffe",
      "metadata": {
        "id": "49f73ffe"
      },
      "source": [
        "\n",
        "## 7. Q-Learning\n",
        "\n",
        "Q-Learning adalah algoritma off-policy untuk belajar Q-Value optimal.\n",
        "\n",
        "Update rule:\n",
        "\n",
        "$Q(s,a) ← Q(s,a) + α [ r + γ max_{a'} Q(s', a') - Q(s,a) ]$\n",
        "\n",
        "Dimana:\n",
        "- α: Learning rate\n",
        "- r: Reward\n",
        "- s': Next state\n",
        "\n",
        "Q-Learning tidak perlu model probabilitas transisi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c8fccc0",
      "metadata": {
        "id": "6c8fccc0"
      },
      "source": [
        "\n",
        "## 8. SARSA\n",
        "\n",
        "SARSA mirip Q-Learning tetapi on-policy: update Q-Value menggunakan aksi yang benar-benar diambil agent.\n",
        "\n",
        "Update rule:\n",
        "\n",
        "$Q(s,a) ← Q(s,a) + α [ r + γ Q(s', a') - Q(s,a) ]$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a5a56b",
      "metadata": {
        "id": "c1a5a56b"
      },
      "source": [
        "\n",
        "## 9. Exploration vs Exploitation\n",
        "\n",
        "Agen harus menyeimbangkan:\n",
        "- Exploration: Mencoba aksi baru untuk pengetahuan baru.\n",
        "- Exploitation: Memilih aksi terbaik berdasarkan pengalaman saat ini.\n",
        "\n",
        "Metode umum:\n",
        "- ε-greedy: Dengan probabilitas ε, pilih aksi acak; sisanya pilih aksi terbaik.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44d9540a",
      "metadata": {
        "id": "44d9540a"
      },
      "source": [
        "\n",
        "## 10. Deep Q-Network (DQN)\n",
        "\n",
        "Untuk state besar/kompleks (misalnya gambar), Q-Value diaproksimasi dengan jaringan saraf. Algoritma ini disebut Deep Q-Network (DQN).\n",
        "\n",
        "Komponen penting DQN:\n",
        "- Experience Replay Buffer: Menyimpan pengalaman untuk sampling acak.\n",
        "- Target Network: Jaringan Q kedua untuk stabilitas update.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72e34248",
      "metadata": {
        "id": "72e34248"
      },
      "source": [
        "\n",
        "## 11. Contoh Kode Q-Learning Sederhana\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2935b9a",
      "metadata": {
        "id": "a2935b9a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "n_states = 5\n",
        "n_actions = 2\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "\n",
        "for episode in range(1000):\n",
        "    state = np.random.randint(0, n_states)\n",
        "    for step in range(50):\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(0, n_actions)\n",
        "        else:\n",
        "            action = np.argmax(Q[state])\n",
        "\n",
        "        next_state = np.random.randint(0, n_states)\n",
        "        reward = np.random.randn()\n",
        "\n",
        "        Q[state, action] += alpha * (\n",
        "            reward + gamma * np.max(Q[next_state]) - Q[state, action]\n",
        "        )\n",
        "\n",
        "        state = next_state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "307e9be0",
      "metadata": {
        "id": "307e9be0"
      },
      "source": [
        "\n",
        "## 12. Aplikasi dan Tantangan\n",
        "\n",
        "RL banyak digunakan pada:\n",
        "- Game dan simulasi (AlphaZero)\n",
        "- Robot navigasi\n",
        "- Sistem kontrol otomatis\n",
        "\n",
        "Tantangan utama:\n",
        "- Data sample inefficiency (butuh banyak iterasi)\n",
        "- Stabilitas training (terutama dengan deep RL)\n",
        "- Eksplorasi di state space besar\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
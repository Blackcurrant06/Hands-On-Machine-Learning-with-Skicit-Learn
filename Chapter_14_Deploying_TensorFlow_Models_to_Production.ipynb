{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM23QYMqGba1aiZW/u0uCcn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 14: Deploying TensorFlow Models to Production\n",
        "\n",
        "## 1. Pendahuluan\n",
        "\n",
        "Bab ini membahas bagaimana cara mendistribusikan dan mengimplementasikan model TensorFlow ke lingkungan produksi. Setelah model dilatih dan divalidasi, tantangan selanjutnya adalah bagaimana melayaninya (serving), menyimpan, mengelola versi, dan melakukan inferensi secara cepat dan andal.\n",
        "\n",
        "Bagian ini juga mencakup cara menggunakan TensorFlow Serving, SavedModel, dan integrasi dengan REST API untuk memudahkan deployment ke aplikasi nyata.\n"
      ],
      "metadata": {
        "id": "oqZjHGExNwYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Menyimpan Model dengan SavedModel\n",
        "\n",
        "TensorFlow menyediakan format universal bernama `SavedModel` yang dapat menyimpan:\n",
        "\n",
        "- Arsitektur model\n",
        "- Bobot terlatih\n",
        "- Informasi tentang signature (input dan output)\n",
        "- Optimizer state (jika perlu melanjutkan training)\n",
        "\n",
        "Contoh cara menyimpan model:"
      ],
      "metadata": {
        "id": "2dNu-SAqNuY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "# Misalnya sudah dilatih...\n",
        "\n",
        "# Menyimpan model ke folder\n",
        "model.save(\"my_model\")\n"
      ],
      "metadata": {
        "id": "ajhhdWCZN2Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk memuatnya kembali:"
      ],
      "metadata": {
        "id": "hlM3zHeNOuJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model(\"my_model\")\n"
      ],
      "metadata": {
        "id": "LaWMbIfuOAIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. TensorFlow Serving\n",
        "\n",
        "TensorFlow Serving adalah sistem fleksibel dan berperforma tinggi untuk menyajikan model machine learning di produksi. Keunggulannya:\n",
        "\n",
        "- Mendukung hot-swapping model (versi lama ke versi baru tanpa downtime)\n",
        "- Mendukung gRPC dan REST API\n",
        "- Mendukung batching request secara otomatis\n",
        "\n",
        "Langkah dasar untuk menggunakan TensorFlow Serving:\n",
        "\n",
        "1. Simpan model menggunakan format SavedModel.\n",
        "2. Jalankan TensorFlow Serving Docker container atau instal binary di server.\n",
        "3. Konfigurasi path ke model.\n",
        "4. Kirim request inferensi via REST API atau gRPC.\n",
        "\n",
        "Contoh command Docker untuk menjalankan TensorFlow Serving:"
      ],
      "metadata": {
        "id": "sg7kGJJHNyxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker run -p 8501:8501 \\\n",
        "  --mount type=bind,source=$(pwd)/my_model,target=/models/my_model \\\n",
        "  -e MODEL_NAME=my_model -t tensorflow/serving\n"
      ],
      "metadata": {
        "id": "uOOnoIdbOI_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model akan tersedia di:\n",
        "\n",
        "* REST API: http://localhost:8501/v1/models/my_model:predict\n",
        "* gRPC: port 8500\n",
        "\n"
      ],
      "metadata": {
        "id": "C0NRokSZO1l7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Batching dan Autotuning\n",
        "\n",
        "TensorFlow Serving dapat melakukan batching request secara otomatis. Ini berguna untuk meningkatkan throughput server inferensi pada beban tinggi.\n",
        "\n",
        "Fitur ini menggabungkan beberapa permintaan menjadi satu batch besar dan mengirimkannya ke model sekaligus. Dengan demikian, inferensi bisa lebih efisien terutama di GPU.\n"
      ],
      "metadata": {
        "id": "FEohaZJ_OLhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Signature dan API\n",
        "\n",
        "Saat menyimpan model, TensorFlow menyimpan signature yang mendefinisikan nama input dan output. Signature mempermudah klien untuk tahu cara mengirim data ke model.\n",
        "\n",
        "Contoh memeriksa signature model:\n"
      ],
      "metadata": {
        "id": "O_jZjpcMON9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model(\"my_model\")\n",
        "model.signatures\n"
      ],
      "metadata": {
        "id": "cDnweM3ROQNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Membuat REST Client\n",
        "\n",
        "Client inferensi bisa dibuat dalam berbagai bahasa. Berikut contoh sederhana menggunakan Python dan `requests` untuk mengirim JSON ke TensorFlow Serving:\n"
      ],
      "metadata": {
        "id": "QrOyGJ6NOSLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "data = json.dumps({\"instances\": [[1.0, 2.0, 5.0, 8.0]]})\n",
        "response = requests.post(\"http://localhost:8501/v1/models/my_model:predict\", data=data)\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "CmqUXxZnOU2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Versioning\n",
        "\n",
        "TensorFlow Serving mendukung multiple version untuk model yang sama. Versi model diletakkan dalam subdirektori:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wORpPUl-OXaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "/models/my_model/1/\n",
        "/models/my_model/2/"
      ],
      "metadata": {
        "id": "7ZcRGwDlPJAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow Serving otomatis akan menggunakan versi terbaru secara default, tetapi klien juga bisa meminta versi tertentu."
      ],
      "metadata": {
        "id": "4oiV-4iLPMM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Monitoring dan Logging\n",
        "\n",
        "Dalam produksi, penting untuk memonitor performa model, latency, throughput, dan error rate. Anda bisa menggunakan tools seperti Prometheus, Grafana, atau Cloud Monitoring. Logging input dan output juga membantu untuk audit dan debugging.\n"
      ],
      "metadata": {
        "id": "uBjfifa5OZD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Alternatif Deployment\n",
        "\n",
        "Selain TensorFlow Serving, ada beberapa alternatif deployment:\n",
        "\n",
        "- Gunakan model di dalam aplikasi Python dengan Flask atau FastAPI.\n",
        "- Deploy ke cloud service seperti Google AI Platform, AWS SageMaker, atau Azure ML.\n",
        "- Export ke format lain seperti TensorFlow Lite atau TensorFlow.js untuk aplikasi mobile dan web.\n"
      ],
      "metadata": {
        "id": "vtWonxgQObUi"
      }
    }
  ]
}